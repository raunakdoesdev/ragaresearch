{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import toml\n",
    "from collections import defaultdict\n",
    "config = toml.load('config.toml')\n",
    "config = defaultdict(lambda : None, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Package Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install pytorch_lightning  # Install pytorch convenience wrapper\n",
    "!pip install attributedict\n",
    "!pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from torch.utils.data import Dataset\n",
    "import copy\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import pickle\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "import plotly.graph_objects as go\n",
    "pio.templates.default = \"plotly_dark\"\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import copy\n",
    "import multiprocessing as mp\n",
    "from argparse import ArgumentParser\n",
    "from collections import OrderedDict\n",
    "import pytorch_lightning as pl\n",
    "import pytorch_lightning.metrics.functional as F\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from sklearn import metrics\n",
    "from sklearn.manifold import TSNE\n",
    "from torch.utils.data import DataLoader\n",
    "from pytorch_lightning.metrics.functional import *\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from attributedict.collections import AttributeDict\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing\n",
    "## Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(config['data']['chroma_folder']):\n",
    "    raise Exception('Error Downloading Dataset!')\n",
    "    if not os.path.exists('chroma.zip'):\n",
    "        !gsutil -m cp gs://ragaresearch/chroma.zip ./\n",
    "    !unzip chroma.zip\n",
    "    !mv n_ftt4096__hop_length2048 hindustani-chroma\n",
    "    !rm chroma.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Chromagram Files: 100%|██████████| 300/300 [00:00<00:00, 1748.33it/s]\n"
     ]
    }
   ],
   "source": [
    "class FullChromaDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset object that returns full length songs as chromagams, accompanied with a \"RAGA ID\" label which is \n",
    "    computed dynamically based on the ragas in the given set in alphabetical order.\n",
    "    \"\"\"\n",
    "\n",
    "    def _assign_raga_ids(self):\n",
    "        \"\"\"\n",
    "        Helper function. Creates a raga to raga id mapping using the list of ragas in alphabetical order.\n",
    "        \"\"\"\n",
    "\n",
    "        mbids = [os.path.basename(file_name).split('.')[0] for file_name in self.files]\n",
    "        raga_ids = {self.metadata[mbid]['raags'][0]['common_name'] for mbid in mbids}\n",
    "        raga_ids = sorted(raga_ids)\n",
    "        self.raga_ids = {k: v for v, k in enumerate(raga_ids)}\n",
    "\n",
    "    def _get_raga_id(self, file):\n",
    "        \"\"\"\n",
    "        Helper function. Gets the raga id associated with a specific chromagram file.\n",
    "        \"\"\"\n",
    "\n",
    "        if not hasattr(self, 'raga_ids') or self.raga_ids is None:\n",
    "            self._assign_raga_ids()\n",
    "        mbid = os.path.basename(file).split('.')[0]\n",
    "        return self.raga_ids[self.metadata[mbid]['raags'][0]['common_name']]\n",
    "\n",
    "    def __init__(self, json_path, data_folder, include_mbids=None):\n",
    "        \"\"\"\n",
    "        Creates a new dataset object.\n",
    "\n",
    "        :param json_path: path to json file with all raga metadata\n",
    "        :param data_folder: folder with all of the chromagrams inside\n",
    "        :param include_mbids: list of song ids to include\n",
    "        \"\"\"\n",
    "        self.files = glob.glob(os.path.join(data_folder, '**/*.pkl'))\n",
    "        self.files += glob.glob(os.path.join(data_folder, '*.pkl'))\n",
    "        self.metadata = json.load(open(json_path, 'r'))\n",
    "\n",
    "        # Remove files not on the \"include\" list (can easily create a subset of the main dataset)\n",
    "        if include_mbids is not None:\n",
    "            for self.file in copy.deepcopy(self.files):\n",
    "                file_name = os.path.basename(self.file).split('.pkl')[0]\n",
    "                if file_name not in include_mbids:\n",
    "                    self.files.remove(self.file)\n",
    "        else:\n",
    "            for self.file in copy.deepcopy(self.files):\n",
    "                mbid = os.path.basename(self.file).split('.')[0]\n",
    "                if len(self.metadata[mbid]['raags']) < 1:\n",
    "                    self.files.remove(self.file)\n",
    "\n",
    "\n",
    "        self.X = []\n",
    "        self.y = []\n",
    "        for file in tqdm(self.files, desc=\"Loading Chromagram Files\"):\n",
    "            self.X.append(torch.FloatTensor(pickle.load(open(file, 'rb'))))\n",
    "            self.y.append(self._get_raga_id(file))\n",
    "\n",
    "    @classmethod\n",
    "    def init_x_y(cls, X, y, raga_ids):\n",
    "        \"\"\"\n",
    "        Helper method. Bypasses default constructor to allow for construction with just X and y objects directly.\n",
    "        \"\"\"\n",
    "        obj = cls.__new__(cls)\n",
    "        obj.X = X\n",
    "        obj.y = y\n",
    "        obj.raga_ids = raga_ids\n",
    "        return obj\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.X[item], self.y[item]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def train_test_split(self, test_size=None, train_size=None, random_state=1):\n",
    "        \"\"\"\n",
    "        Creates two new datasets from the original dataset object by splitting the datasets in a stratified fashion.\n",
    "\n",
    "        :param test_size: size of test set (as a percentage)\n",
    "        :param train_size: size of the train set (as a percentage)\n",
    "        :param random_state: random seed used to shuffle the data before splitting\n",
    "        \"\"\"\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(self.X, self.y, test_size=test_size, train_size=train_size,\n",
    "                                                            stratify=self.y, random_state=random_state)\n",
    "        return FullChromaDataset.init_x_y(X_train, y_train, self.raga_ids), FullChromaDataset.init_x_y(X_test, y_test, self.raga_ids)\n",
    "    \n",
    "    \n",
    "\n",
    "    def greedy_split(self, train_size=None, test_size=None):\n",
    "        if test_size is None:\n",
    "            test_size = 1 - train_size\n",
    "        else:\n",
    "            train_size = 1 - test_size\n",
    "\n",
    "        # Split Samples by Raga\n",
    "        samples_by_raga = [[] for i in range(len(self.raga_ids))]\n",
    "        for X, y in self:\n",
    "            samples_by_raga[y].append(X)\n",
    "        X_train, y_train = [], []\n",
    "        X_test, y_test = [], []\n",
    "        for raga, samples in enumerate(samples_by_raga):\n",
    "            train_len, test_len = 0, 0\n",
    "            for sample in sorted(samples, reverse=True, key=lambda sample: len(sample[0])):\n",
    "                if train_len <= test_len:\n",
    "                    X_train.append(sample)\n",
    "                    y_train.append(raga)\n",
    "                    train_len += len(sample[0]) * (test_size/train_size)\n",
    "                else:\n",
    "                    X_test.append(sample)\n",
    "                    y_test.append(raga)\n",
    "                    test_len += len(sample[0])\n",
    "\n",
    "        return FullChromaDataset.init_x_y(X_train, y_train, self.raga_ids), FullChromaDataset.init_x_y(X_test, y_test, self.raga_ids)\n",
    "        \n",
    "\n",
    "\n",
    "if config['data']['unit_test']:\n",
    "    fcd = FullChromaDataset(json_path=config['data']['metadata'],\n",
    "                            data_folder=config['data']['chroma_folder'],\n",
    "                            include_mbids=json.load(open(config['data']['limit_songs'])))\n",
    "    fcd_train, fcd_val = fcd.greedy_split(train_size=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChromaChunkDataset(Dataset):\n",
    "    def __init__(self, full_chroma_dataset: FullChromaDataset, chunk_size, augmentation=None):\n",
    "        \"\"\"\n",
    "        Class for chunkifying an existing full chroma dataset\n",
    "\n",
    "        :param full_chroma_dataset: FullChromaDataset object\n",
    "        :param chunk_size: size of the chunks to make from the original set\n",
    "        :param augmentation: function that the chunks are passed through before calling get_item (user defined)\n",
    "        \"\"\"\n",
    "\n",
    "        self.X = []\n",
    "        self.y = []\n",
    "        self.augmentation = augmentation\n",
    "        self.raga_ids = full_chroma_dataset.raga_ids\n",
    "\n",
    "        for chroma, raga_id in full_chroma_dataset:\n",
    "            unfolded = chroma.split(chunk_size, dim=1)\n",
    "            for i in range(len(unfolded)):\n",
    "                chroma = unfolded[i]\n",
    "                if unfolded[i].shape[1] != chunk_size:\n",
    "                    padding = torch.zeros(unfolded[i].shape[0], chunk_size - unfolded[i].shape[1])\n",
    "                    chroma = torch.cat((unfolded[i], padding), 1)\n",
    "                self.X.append(chroma.unsqueeze(0))\n",
    "            self.y += len(unfolded) * [raga_id]\n",
    "\n",
    "        self.X = torch.cat(self.X, dim=0)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        if self.augmentation is None:\n",
    "            return self.X[item], self.y[item]\n",
    "        else:\n",
    "            return self.augmentation(self.X[item]), self.y[item]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "if config['data']['unit_test']:\n",
    "    fcd_train_chunks =  ChromaChunkDataset(fcd_train, chunk_size=config['data']['chunk_size'])\n",
    "    assert(len(fcd_train_chunks) > len(fcd_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raga_list(dataset):\n",
    "    \"\"\"\n",
    "    Accesses a FullChromaDataset or ChunkChromaDataset object and extracts list of ragas in order of raga id\n",
    "    \"\"\"\n",
    "    return sorted(dataset.raga_ids)\n",
    "\n",
    "if config['data']['unit_test']:\n",
    "    assert 'Bhairav' in get_raga_list(fcd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transpose_chromagram(x, shift=None):\n",
    "    if shift is None:\n",
    "        shift = random.randint(0, 11)\n",
    "    if shift == 0:\n",
    "        return x\n",
    "    else:\n",
    "        return torch.cat([x[-shift:, :], x[:-shift, :]], 0)\n",
    "\n",
    "if config['data']['unit_test']:\n",
    "    pass #TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balanced Dataloader\n",
    "There can be class imbalance from the creation of chunks, so it's valuable to have a data loader which samples based on the prevalance of the classes. This is code adapted from https://github.com/ufoym/imbalanced-dataset-sampler for taking care of the balanced sampling :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_get_label_callback(dataset, idx):\n",
    "    return dataset[idx][1]\n",
    "\n",
    "\n",
    "class ImbalancedDatasetSampler(torch.utils.data.sampler.Sampler):\n",
    "    \"\"\"Samples elements randomly from a given list of indices for imbalanced dataset\n",
    "    Arguments:\n",
    "        indices (list, optional): a list of indices\n",
    "        num_samples (int, optional): number of samples to draw\n",
    "        callback_get_label func: a callback-like function which takes two arguments - dataset and index\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset, indices=None, num_samples=None, callback_get_label=dataset_get_label_callback):\n",
    "\n",
    "        # if indices is not provided,\n",
    "        # all elements in the dataset will be considered\n",
    "        self.indices = list(range(len(dataset))) \\\n",
    "            if indices is None else indices\n",
    "\n",
    "        # define custom callback\n",
    "        self.callback_get_label = callback_get_label\n",
    "\n",
    "        # if num_samples is not provided,\n",
    "        # draw `len(indices)` samples in each iteration\n",
    "        self.num_samples = len(self.indices) \\\n",
    "            if num_samples is None else num_samples\n",
    "\n",
    "        # distribution of classes in the dataset\n",
    "        label_to_count = {}\n",
    "        for idx in self.indices:\n",
    "            label = self._get_label(dataset, idx)\n",
    "            if label in label_to_count:\n",
    "                label_to_count[label] += 1\n",
    "            else:\n",
    "                label_to_count[label] = 1\n",
    "\n",
    "        # weight for each sample\n",
    "        weights = [1.0 / label_to_count[self._get_label(dataset, idx)]\n",
    "                   for idx in self.indices]\n",
    "        self.weights = torch.DoubleTensor(weights)\n",
    "\n",
    "    def _get_label(self, dataset, idx):\n",
    "        if isinstance(dataset, torchvision.datasets.MNIST):\n",
    "            return dataset.train_labels[idx].item()\n",
    "        elif isinstance(dataset, torchvision.datasets.ImageFolder):\n",
    "            return dataset.imgs[idx][1]\n",
    "        elif isinstance(dataset, torch.utils.data.Subset):\n",
    "            return dataset.dataset.imgs[idx][1]\n",
    "        elif self.callback_get_label:\n",
    "            return self.callback_get_label(dataset, idx)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def __iter__(self):\n",
    "        return (self.indices[i] for i in torch.multinomial(\n",
    "            self.weights, self.num_samples, replacement=True))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-4.m50",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m50"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
